{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Predicao do Preco de Fechamento da PETR4.SA com LSTM\n\n## Projeto de Deep Learning aplicado a Series Temporais Financeiras\n\n**Autor:** Guilherme de Mauro Favaron\n\n---\n\n### Sobre este notebook\n\nEste notebook implementa um modelo preditivo de redes neurais **Long Short-Term Memory (LSTM)** para prever o preco de fechamento das acoes da **Petrobras (PETR4.SA)** na bolsa de valores brasileira.\n\nO objetivo e construir toda a pipeline de desenvolvimento, desde a coleta dos dados ate a exportacao do modelo treinado para uso em producao.\n\n### O que voce vai aprender aqui\n\n1. Como coletar dados financeiros usando a biblioteca `yfinance`\n2. Como preparar dados de series temporais para alimentar uma LSTM\n3. Como construir e treinar um modelo LSTM em PyTorch\n4. Como avaliar o modelo com metricas apropriadas (MAE, RMSE, MAPE)\n5. Como exportar o modelo treinado para uso em uma API\n\n### Pre-requisitos\n\n- Conhecimento basico de Python\n- Nocoes de Machine Learning (o que e treino, validacao, teste)\n- Este notebook roda no **Google Colab** com GPU gratuita (tambem funciona localmente com CUDA ou Apple Silicon MPS)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuracao do Ambiente\n",
    "\n",
    "Primeiro, vamos instalar e importar todas as bibliotecas necessarias.\n",
    "\n",
    "**Por que essas bibliotecas?**\n",
    "- `yfinance`: faz download de dados historicos de acoes direto do Yahoo Finance\n",
    "- `torch` (PyTorch): framework de deep learning que usaremos para construir a LSTM\n",
    "- `sklearn`: usaremos o `MinMaxScaler` para normalizar os dados\n",
    "- `matplotlib` / `plotly`: para visualizacoes graficas\n",
    "- `pandas` / `numpy`: manipulacao de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Instalacao das dependencias (necessario no Colab)\n!pip install yfinance plotly -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport yfinance as yf\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport joblib\nimport json\nimport os\nimport warnings\nimport time\nfrom datetime import datetime\n\nwarnings.filterwarnings('ignore')\n\n# Detectar melhor dispositivo disponivel:\n# - CUDA para GPUs NVIDIA (Colab T4, etc.)\n# - MPS para Apple Silicon (M1/M2/M3)\n# - CPU como fallback\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(f'Dispositivo: {device} ({torch.cuda.get_device_name(0)})')\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = torch.device('mps')\n    print(f'Dispositivo: {device} (Apple Silicon GPU)')\nelse:\n    device = torch.device('cpu')\n    print(f'Dispositivo: {device}')\n\n# Seed para reproducibilidade\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coleta dos Dados\n",
    "\n",
    "Vamos baixar o historico de precos da **PETR4.SA** (Petrobras) usando a biblioteca `yfinance`.\n",
    "\n",
    "**O que sao esses dados?**\n",
    "Cada linha representa um dia de negociacao na bolsa e contem:\n",
    "- **Open**: preco de abertura do dia\n",
    "- **High**: preco mais alto atingido no dia\n",
    "- **Low**: preco mais baixo atingido no dia\n",
    "- **Close**: preco de fechamento do dia (nosso **alvo de predicao**)\n",
    "- **Volume**: quantidade de acoes negociadas\n",
    "- **Adj Close**: preco de fechamento ajustado (considera dividendos e splits)\n",
    "\n",
    "Usaremos dados de **janeiro de 2018 ate hoje**, o que nos da varios anos de historico para o modelo aprender padroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracao da coleta\n",
    "SYMBOL = 'PETR4.SA'       # Ticker da Petrobras na B3\n",
    "START_DATE = '2018-01-01'  # Data inicial\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')  # Ate hoje\n",
    "\n",
    "print(f'Baixando dados de {SYMBOL}...')\n",
    "print(f'Periodo: {START_DATE} ate {END_DATE}')\n",
    "\n",
    "# Download dos dados\n",
    "df = yf.download(SYMBOL, start=START_DATE, end=END_DATE)\n",
    "\n",
    "# Flatten MultiIndex columns if present (yfinance >= 0.2.31)\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "print(f'\\nDados baixados com sucesso!')\n",
    "print(f'Total de registros: {len(df)}')\n",
    "print(f'Periodo coberto: {df.index.min().date()} ate {df.index.max().date()}')\n",
    "print(f'\\nPrimeiras linhas:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacoes gerais do dataset\n",
    "print('=== Informacoes do Dataset ===')\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'\\nTipos de dados:')\n",
    "print(df.dtypes)\n",
    "print(f'\\nValores nulos por coluna:')\n",
    "print(df.isnull().sum())\n",
    "print(f'\\nEstatisticas descritivas:')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analise Exploratoria dos Dados (EDA)\n",
    "\n",
    "Antes de construir o modelo, precisamos entender os dados. Vamos visualizar:\n",
    "1. A evolucao do preco ao longo do tempo\n",
    "2. O volume de negociacao\n",
    "3. A distribuicao dos retornos diarios\n",
    "4. A volatilidade\n",
    "\n",
    "Isso nos ajuda a entender o comportamento da serie temporal e validar se uma LSTM e uma boa abordagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico interativo com Plotly: Preco de fechamento + Volume\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.08,\n",
    "    row_heights=[0.7, 0.3],\n",
    "    subplot_titles=(\n",
    "        f'Preco de Fechamento - {SYMBOL}',\n",
    "        'Volume de Negociacao'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Preco de fechamento\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df.index, y=df['Close'],\n",
    "        mode='lines', name='Close',\n",
    "        line=dict(color='#1f77b4', width=1.5)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Media movel de 50 dias\n",
    "df['MA50'] = df['Close'].rolling(window=50).mean()\n",
    "df['MA200'] = df['Close'].rolling(window=200).mean()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df.index, y=df['MA50'],\n",
    "        mode='lines', name='Media Movel 50d',\n",
    "        line=dict(color='orange', width=1, dash='dash')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df.index, y=df['MA200'],\n",
    "        mode='lines', name='Media Movel 200d',\n",
    "        line=dict(color='red', width=1, dash='dash')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Volume\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=df.index, y=df['Volume'],\n",
    "        name='Volume',\n",
    "        marker_color='rgba(31, 119, 180, 0.4)'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    showlegend=True,\n",
    "    title_text=f'Analise Historica - {SYMBOL}',\n",
    "    xaxis2_title='Data',\n",
    "    yaxis_title='Preco (BRL)',\n",
    "    yaxis2_title='Volume'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retornos diarios (variacao percentual dia a dia)\n",
    "df['Returns'] = df['Close'].pct_change()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histograma dos retornos\n",
    "axes[0].hist(df['Returns'].dropna(), bins=80, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "axes[0].set_title('Distribuicao dos Retornos Diarios')\n",
    "axes[0].set_xlabel('Retorno Diario')\n",
    "axes[0].set_ylabel('Frequencia')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Volatilidade (rolling std dos retornos)\n",
    "volatility = df['Returns'].rolling(window=30).std() * np.sqrt(252)  # Anualizada\n",
    "axes[1].plot(df.index, volatility, color='tomato', linewidth=1)\n",
    "axes[1].set_title('Volatilidade Anualizada (janela de 30 dias)')\n",
    "axes[1].set_xlabel('Data')\n",
    "axes[1].set_ylabel('Volatilidade')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Retorno medio diario: {df[\"Returns\"].mean():.4f} ({df[\"Returns\"].mean()*100:.2f}%)')\n",
    "print(f'Desvio padrao dos retornos: {df[\"Returns\"].std():.4f}')\n",
    "print(f'Retorno acumulado no periodo: {((df[\"Close\"].iloc[-1] / df[\"Close\"].iloc[0]) - 1)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-processamento dos Dados\n",
    "\n",
    "Agora vamos preparar os dados para alimentar a LSTM. Essa etapa e **crucial** e envolve:\n",
    "\n",
    "### 4.1 Por que normalizar?\n",
    "Redes neurais funcionam melhor quando os dados estao numa escala entre 0 e 1. O `MinMaxScaler` faz essa transformacao. Sem isso, o modelo teria dificuldade para convergir.\n",
    "\n",
    "### 4.2 O que sao janelas deslizantes (sliding windows)?\n",
    "A LSTM precisa receber uma **sequencia** de valores passados para prever o proximo. Por exemplo, se usarmos uma janela de 60 dias:\n",
    "- **Entrada (X)**: precos dos dias 1 ao 60\n",
    "- **Saida (y)**: preco do dia 61\n",
    "- Depois: entrada = dias 2 ao 61, saida = dia 62, e assim por diante.\n",
    "\n",
    "### 4.3 Split temporal\n",
    "Em series temporais, **nunca embaralhamos os dados**. O split deve respeitar a ordem cronologica:\n",
    "- **Treino**: primeiros 70% dos dados\n",
    "- **Validacao**: proximos 15%\n",
    "- **Teste**: ultimos 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACOES DO PRE-PROCESSAMENTO ===\n",
    "SEQUENCE_LENGTH = 60   # Quantos dias passados usamos para prever o proximo\n",
    "FEATURE_COLUMNS = ['Close']  # Coluna(s) que usaremos como features\n",
    "TARGET_COLUMN = 'Close'      # Coluna que queremos prever\n",
    "\n",
    "# Proporcion do split\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "print(f'Janela de lookback: {SEQUENCE_LENGTH} dias')\n",
    "print(f'Features: {FEATURE_COLUMNS}')\n",
    "print(f'Target: {TARGET_COLUMN}')\n",
    "print(f'Split: {TRAIN_RATIO*100:.0f}% treino / {VAL_RATIO*100:.0f}% validacao / {TEST_RATIO*100:.0f}% teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar apenas a coluna de fechamento e remover nulos\n",
    "data = df[FEATURE_COLUMNS].dropna().values\n",
    "\n",
    "print(f'Total de amostras disponiveis: {len(data)}')\n",
    "\n",
    "# === NORMALIZACAO ===\n",
    "# O MinMaxScaler transforma os valores para o intervalo [0, 1]\n",
    "# Exemplo: se o preco varia de R$10 a R$40, R$25 vira 0.5\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "print(f'Valor minimo original: R${data.min():.2f}')\n",
    "print(f'Valor maximo original: R${data.max():.2f}')\n",
    "print(f'Apos normalizacao: min={data_scaled.min():.4f}, max={data_scaled.max():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\"\n",
    "    Cria sequencias de entrada (X) e saidas (y) para a LSTM.\n",
    "    \n",
    "    Parametros:\n",
    "        data: array normalizado com os precos\n",
    "        sequence_length: quantos dias passados usar como entrada\n",
    "    \n",
    "    Retorna:\n",
    "        X: array de shape (n_amostras, sequence_length, n_features)\n",
    "        y: array de shape (n_amostras, 1)\n",
    "    \n",
    "    Exemplo com sequence_length=3 e dados [10, 20, 30, 40, 50]:\n",
    "        X[0] = [10, 20, 30] -> y[0] = 40\n",
    "        X[1] = [20, 30, 40] -> y[1] = 50\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:(i + sequence_length)])\n",
    "        y.append(data[i + sequence_length, 0])  # Coluna 0 = Close\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Criar as sequencias\n",
    "X, y = create_sequences(data_scaled, SEQUENCE_LENGTH)\n",
    "\n",
    "print(f'Shape de X: {X.shape}  (amostras, janela_temporal, features)')\n",
    "print(f'Shape de y: {y.shape}  (amostras,)')\n",
    "print(f'\\nExemplo: para prever o preco do dia {SEQUENCE_LENGTH+1},') \n",
    "print(f'o modelo recebe os precos (normalizados) dos {SEQUENCE_LENGTH} dias anteriores.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SPLIT TEMPORAL ===\n",
    "# IMPORTANTE: nao embaralhamos! A ordem cronologica deve ser mantida.\n",
    "\n",
    "n_total = len(X)\n",
    "n_train = int(n_total * TRAIN_RATIO)\n",
    "n_val = int(n_total * VAL_RATIO)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_val, y_val = X[n_train:n_train+n_val], y[n_train:n_train+n_val]\n",
    "X_test, y_test = X[n_train+n_val:], y[n_train+n_val:]\n",
    "\n",
    "print(f'Conjunto de Treino:    {len(X_train)} amostras ({len(X_train)/n_total*100:.1f}%)')\n",
    "print(f'Conjunto de Validacao: {len(X_val)} amostras ({len(X_val)/n_total*100:.1f}%)')\n",
    "print(f'Conjunto de Teste:     {len(X_test)} amostras ({len(X_test)/n_total*100:.1f}%)')\n",
    "\n",
    "# Converter para tensores PyTorch\n",
    "X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "y_train_t = torch.FloatTensor(y_train).to(device)\n",
    "X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "y_val_t = torch.FloatTensor(y_val).to(device)\n",
    "X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# Criar DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f'\\nBatch size: {BATCH_SIZE}')\n",
    "print(f'Batches de treino: {len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construcao do Modelo LSTM\n",
    "\n",
    "### O que e uma LSTM?\n",
    "\n",
    "**LSTM (Long Short-Term Memory)** e um tipo especial de rede neural recorrente (RNN) projetada para aprender dependencias de longo prazo em sequencias de dados.\n",
    "\n",
    "Uma LSTM possui tres \"portoes\" (gates) que controlam o fluxo de informacao:\n",
    "- **Forget Gate**: decide o que esquecer do estado anterior\n",
    "- **Input Gate**: decide quais novas informacoes armazenar\n",
    "- **Output Gate**: decide o que enviar como saida\n",
    "\n",
    "Isso permite que a rede \"lembre\" padroes de muitos dias atras, algo essencial para series temporais financeiras.\n",
    "\n",
    "### Arquitetura do nosso modelo\n",
    "\n",
    "```\n",
    "Input (60 dias, 1 feature)\n",
    "    |\n",
    "    v\n",
    "LSTM Layer 1 (128 unidades) + Dropout(0.2)\n",
    "    |\n",
    "    v\n",
    "LSTM Layer 2 (64 unidades) + Dropout(0.2)\n",
    "    |\n",
    "    v\n",
    "Fully Connected (64 -> 32)\n",
    "    |\n",
    "    v\n",
    "Output (32 -> 1) = Preco previsto\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo LSTM para predicao de series temporais financeiras.\n",
    "    \n",
    "    Parametros:\n",
    "        input_size: numero de features de entrada (1 = apenas Close)\n",
    "        hidden_size: numero de neuronios na camada LSTM\n",
    "        num_layers: numero de camadas LSTM empilhadas\n",
    "        dropout: taxa de dropout para regularizacao (evita overfitting)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Camadas LSTM empilhadas\n",
    "        # batch_first=True: formato de entrada sera (batch, sequencia, features)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Camadas fully connected para gerar a previsao final\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inicializar estados ocultos com zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Passar pela LSTM\n",
    "        # lstm_out contem as saidas de todos os timesteps\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Pegar apenas a saida do ultimo timestep\n",
    "        # (queremos a previsao baseada em toda a sequencia)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Passar pelas camadas fully connected\n",
    "        prediction = self.fc(last_output)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === HIPERPARAMETROS ===\nHIDDEN_SIZE = 128      # Neuronios na LSTM\nNUM_LAYERS = 2         # Camadas LSTM\nDROPOUT = 0.2          # Taxa de dropout\nLEARNING_RATE = 0.001  # Taxa de aprendizado\nNUM_EPOCHS = 100       # Numero maximo de epocas\nPATIENCE = 15          # Early stopping: para se nao melhorar em 15 epocas\n\n# Criar o modelo\nmodel = LSTMModel(\n    input_size=len(FEATURE_COLUMNS),\n    hidden_size=HIDDEN_SIZE,\n    num_layers=NUM_LAYERS,\n    dropout=DROPOUT\n).to(device)\n\n# Funcao de perda e otimizador\ncriterion = nn.MSELoss()  # Mean Squared Error\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Learning rate scheduler: reduz o LR quando o modelo para de melhorar\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=5\n)\n\n# Resumo do modelo\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint('=== Arquitetura do Modelo ===')\nprint(model)\nprint(f'\\nTotal de parametros: {total_params:,}')\nprint(f'Parametros treinaveis: {trainable_params:,}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Treinamento do Modelo\n",
    "\n",
    "Agora vamos treinar o modelo. Durante o treinamento:\n",
    "\n",
    "1. O modelo recebe batches de sequencias de 60 dias\n",
    "2. Faz uma previsao para o dia seguinte\n",
    "3. Compara a previsao com o valor real (calcula a perda/loss)\n",
    "4. Ajusta os pesos via backpropagation\n",
    "5. Repete para todos os batches (1 epoca)\n",
    "\n",
    "Usamos **Early Stopping**: se a perda no conjunto de validacao nao melhorar por 15 epocas seguidas, paramos o treinamento para evitar overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                num_epochs, patience, device):\n",
    "    \"\"\"\n",
    "    Treina o modelo LSTM com early stopping.\n",
    "    \n",
    "    Retorna:\n",
    "        model: modelo com os melhores pesos\n",
    "        history: dicionario com historico de loss de treino e validacao\n",
    "    \"\"\"\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # === TREINO ===\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch).squeeze()\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping para estabilidade\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        # === VALIDACAO ===\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                predictions = model(X_batch).squeeze()\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        # Registrar historico\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Atualizar learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            epochs_without_improvement = 0\n",
    "            marker = ' <-- melhor'\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            marker = ''\n",
    "        \n",
    "        # Log a cada 10 epocas ou na ultima\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or marker:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'Epoca [{epoch+1:3d}/{num_epochs}] '\n",
    "                  f'Train Loss: {avg_train_loss:.6f} | '\n",
    "                  f'Val Loss: {avg_val_loss:.6f} | '\n",
    "                  f'LR: {optimizer.param_groups[0][\"lr\"]:.6f} | '\n",
    "                  f'Tempo: {elapsed:.0f}s{marker}')\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'\\nEarly stopping na epoca {epoch+1}! '\n",
    "                  f'Sem melhoria ha {patience} epocas.')\n",
    "            break\n",
    "    \n",
    "    # Carregar os melhores pesos\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f'\\nMelhores pesos restaurados (val_loss: {best_val_loss:.6f})')\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f'Treinamento concluido em {total_time:.1f} segundos.')\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# === TREINAR ===\n",
    "print('Iniciando treinamento...\\n')\n",
    "model, history = train_model(\n",
    "    model, train_loader, val_loader,\n",
    "    criterion, optimizer, scheduler,\n",
    "    NUM_EPOCHS, PATIENCE, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curva de treinamento\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(history['train_loss'], label='Treino', color='steelblue', linewidth=2)\n",
    "ax.plot(history['val_loss'], label='Validacao', color='tomato', linewidth=2)\n",
    "ax.set_title('Curva de Treinamento (Loss)', fontsize=14)\n",
    "ax.set_xlabel('Epoca')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Loss final de treino: {history[\"train_loss\"][-1]:.6f}')\n",
    "print(f'Melhor loss de validacao: {min(history[\"val_loss\"]):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Avaliacao do Modelo\n",
    "\n",
    "Vamos avaliar o modelo no **conjunto de teste** (dados que ele nunca viu durante o treinamento).\n",
    "\n",
    "Usaremos tres metricas:\n",
    "\n",
    "- **MAE (Mean Absolute Error)**: media dos erros absolutos. Ex: MAE = 1.5 significa que, em media, a previsao erra R$1.50.\n",
    "- **RMSE (Root Mean Square Error)**: raiz da media dos erros ao quadrado. Penaliza mais os erros grandes.\n",
    "- **MAPE (Mean Absolute Percentage Error)**: erro medio em percentual. Ex: MAPE = 3% significa que a previsao erra, em media, 3% do valor real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_tensor, y_tensor, scaler, device):\n",
    "    \"\"\"\n",
    "    Avalia o modelo e retorna previsoes desnormalizadas + metricas.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_scaled = model(X_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    y_true_scaled = y_tensor.cpu().numpy().flatten()\n",
    "    \n",
    "    # Desnormalizar: voltar para a escala original (BRL)\n",
    "    # Precisamos reshape para o scaler aceitar\n",
    "    predictions_real = scaler.inverse_transform(\n",
    "        predictions_scaled.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    y_true_real = scaler.inverse_transform(\n",
    "        y_true_scaled.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    # Calcular metricas\n",
    "    mae = mean_absolute_error(y_true_real, predictions_real)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_real, predictions_real))\n",
    "    mape = np.mean(np.abs((y_true_real - predictions_real) / y_true_real)) * 100\n",
    "    \n",
    "    metrics = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
    "    \n",
    "    return predictions_real, y_true_real, metrics\n",
    "\n",
    "# Avaliar no conjunto de teste\n",
    "test_pred, test_true, test_metrics = evaluate_model(\n",
    "    model, X_test_t, y_test_t, scaler, device\n",
    ")\n",
    "\n",
    "print('=== METRICAS NO CONJUNTO DE TESTE ===')\n",
    "print(f'MAE  (Erro Absoluto Medio):     R$ {test_metrics[\"MAE\"]:.2f}')\n",
    "print(f'RMSE (Raiz do Erro Quadratico): R$ {test_metrics[\"RMSE\"]:.2f}')\n",
    "print(f'MAPE (Erro Percentual Medio):   {test_metrics[\"MAPE\"]:.2f}%')\n",
    "\n",
    "# Avaliar tambem treino e validacao para comparacao\n",
    "train_pred, train_true, train_metrics = evaluate_model(\n",
    "    model, X_train_t, y_train_t, scaler, device\n",
    ")\n",
    "val_pred, val_true, val_metrics = evaluate_model(\n",
    "    model, X_val_t, y_val_t, scaler, device\n",
    ")\n",
    "\n",
    "print(f'\\n=== COMPARATIVO ===')\n",
    "print(f'{\"Metrica\":<8} {\"Treino\":>10} {\"Validacao\":>10} {\"Teste\":>10}')\n",
    "print(f'{\"-\"*40}')\n",
    "for m in ['MAE', 'RMSE', 'MAPE']:\n",
    "    unit = '%' if m == 'MAPE' else ''\n",
    "    prefix = '' if m == 'MAPE' else 'R$ '\n",
    "    print(f'{m:<8} {prefix}{train_metrics[m]:>8.2f}{unit} {prefix}{val_metrics[m]:>8.2f}{unit} {prefix}{test_metrics[m]:>8.2f}{unit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GRAFICO: Real vs Previsto (Teste) ===\n",
    "fig = go.Figure()\n",
    "\n",
    "# Obter as datas correspondentes ao conjunto de teste\n",
    "test_dates = df.index[n_train + n_val + SEQUENCE_LENGTH:n_train + n_val + SEQUENCE_LENGTH + len(test_true)]\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=test_dates, y=test_true,\n",
    "    mode='lines', name='Valor Real',\n",
    "    line=dict(color='#1f77b4', width=2)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=test_dates, y=test_pred,\n",
    "    mode='lines', name='Previsao LSTM',\n",
    "    line=dict(color='#ff7f0e', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Conjunto de Teste: Real vs Previsto - {SYMBOL}',\n",
    "    xaxis_title='Data',\n",
    "    yaxis_title='Preco (BRL)',\n",
    "    height=500,\n",
    "    legend=dict(yanchor='top', y=0.99, xanchor='left', x=0.01)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === GRAFICO COMPLETO: Treino + Validacao + Teste ===\nfig = go.Figure()\n\n# Datas para cada conjunto\ntrain_dates = df.index[SEQUENCE_LENGTH:SEQUENCE_LENGTH + len(train_true)]\nval_dates = df.index[n_train + SEQUENCE_LENGTH:n_train + SEQUENCE_LENGTH + len(val_true)]\n\n# Valores reais completos\nall_dates = df.index[SEQUENCE_LENGTH:SEQUENCE_LENGTH + len(train_true) + len(val_true) + len(test_true)]\nall_true = np.concatenate([train_true, val_true, test_true])\n\nfig.add_trace(go.Scatter(\n    x=all_dates, y=all_true,\n    mode='lines', name='Valor Real',\n    line=dict(color='#1f77b4', width=1.5)\n))\n\n# Previsoes por conjunto\nfig.add_trace(go.Scatter(\n    x=train_dates, y=train_pred,\n    mode='lines', name='Previsao (Treino)',\n    line=dict(color='green', width=1, dash='dot'),\n    opacity=0.6\n))\n\nfig.add_trace(go.Scatter(\n    x=val_dates, y=val_pred,\n    mode='lines', name='Previsao (Validacao)',\n    line=dict(color='orange', width=1.5, dash='dash')\n))\n\nfig.add_trace(go.Scatter(\n    x=test_dates, y=test_pred,\n    mode='lines', name='Previsao (Teste)',\n    line=dict(color='red', width=2, dash='dash')\n))\n\n# Linhas verticais separando os conjuntos\n# Converter para string ISO para evitar incompatibilidade Plotly + Pandas Timestamp\nfig.add_vline(x=str(val_dates[0]), line_dash='dash', line_color='gray', opacity=0.5,\n              annotation_text='Inicio Validacao')\nfig.add_vline(x=str(test_dates[0]), line_dash='dash', line_color='gray', opacity=0.5,\n              annotation_text='Inicio Teste')\n\nfig.update_layout(\n    title=f'Visao Completa: Real vs Previsto - {SYMBOL}',\n    xaxis_title='Data',\n    yaxis_title='Preco (BRL)',\n    height=600,\n    legend=dict(yanchor='top', y=0.99, xanchor='left', x=0.01)\n)\n\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Salvamento e Exportacao do Modelo\n",
    "\n",
    "Agora que temos um modelo treinado e avaliado, vamos salvar tudo que e necessario para usa-lo em producao:\n",
    "\n",
    "1. **Pesos do modelo** (`lstm_model.pth`): os parametros aprendidos pela rede\n",
    "2. **Scaler** (`scaler.joblib`): o MinMaxScaler ajustado, necessario para normalizar novos dados\n",
    "3. **Configuracao** (`config.json`): hiperparametros e metricas, para reconstruir o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar diretorio de saida\n",
    "MODELS_DIR = 'models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Salvar pesos do modelo\n",
    "model_path = os.path.join(MODELS_DIR, 'lstm_model.pth')\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Modelo salvo em: {model_path}')\n",
    "\n",
    "# 2. Salvar o scaler\n",
    "scaler_path = os.path.join(MODELS_DIR, 'scaler.joblib')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f'Scaler salvo em: {scaler_path}')\n",
    "\n",
    "# 3. Salvar configuracao completa\n",
    "config = {\n",
    "    'model': {\n",
    "        'input_size': len(FEATURE_COLUMNS),\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'dropout': DROPOUT,\n",
    "        'sequence_length': SEQUENCE_LENGTH\n",
    "    },\n",
    "    'training': {\n",
    "        'symbol': SYMBOL,\n",
    "        'start_date': START_DATE,\n",
    "        'end_date': END_DATE,\n",
    "        'feature_columns': FEATURE_COLUMNS,\n",
    "        'target_column': TARGET_COLUMN,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_epochs_run': len(history['train_loss']),\n",
    "        'train_ratio': TRAIN_RATIO,\n",
    "        'val_ratio': VAL_RATIO,\n",
    "        'test_ratio': TEST_RATIO\n",
    "    },\n",
    "    'metrics': {\n",
    "        'test': {k: round(v, 4) for k, v in test_metrics.items()},\n",
    "        'validation': {k: round(v, 4) for k, v in val_metrics.items()},\n",
    "        'train': {k: round(v, 4) for k, v in train_metrics.items()}\n",
    "    },\n",
    "    'data_info': {\n",
    "        'total_samples': len(data),\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'scaler_min': float(scaler.data_min_[0]),\n",
    "        'scaler_max': float(scaler.data_max_[0])\n",
    "    },\n",
    "    'exported_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "config_path = os.path.join(MODELS_DIR, 'config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f'Configuracao salva em: {config_path}')\n",
    "\n",
    "# Verificar tamanhos\n",
    "for fname in os.listdir(MODELS_DIR):\n",
    "    fpath = os.path.join(MODELS_DIR, fname)\n",
    "    size_kb = os.path.getsize(fpath) / 1024\n",
    "    print(f'  {fname}: {size_kb:.1f} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VERIFICACAO: carregar o modelo salvo e confirmar que funciona ===\n",
    "\n",
    "# Reconstruir o modelo do zero\n",
    "model_loaded = LSTMModel(\n",
    "    input_size=config['model']['input_size'],\n",
    "    hidden_size=config['model']['hidden_size'],\n",
    "    num_layers=config['model']['num_layers'],\n",
    "    dropout=config['model']['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Carregar os pesos salvos\n",
    "model_loaded.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "model_loaded.eval()\n",
    "\n",
    "# Testar com uma amostra\n",
    "with torch.no_grad():\n",
    "    sample_input = X_test_t[:1]  # Primeira amostra do teste\n",
    "    prediction_original = model(sample_input).item()\n",
    "    prediction_loaded = model_loaded(sample_input).item()\n",
    "\n",
    "# Desnormalizar\n",
    "pred_brl = scaler.inverse_transform([[prediction_loaded]])[0][0]\n",
    "\n",
    "print('=== Verificacao do Modelo Salvo ===')\n",
    "print(f'Previsao modelo original: {prediction_original:.6f}')\n",
    "print(f'Previsao modelo carregado: {prediction_loaded:.6f}')\n",
    "print(f'Previsoes identicas: {abs(prediction_original - prediction_loaded) < 1e-6}')\n",
    "print(f'Valor previsto (BRL): R$ {pred_brl:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Conclusao\n\n### O que fizemos neste notebook:\n\n1. **Coletamos** dados historicos da PETR4.SA usando o Yahoo Finance\n2. **Exploramos** os dados com graficos de preco, volume, retornos e volatilidade\n3. **Pre-processamos** os dados: normalizacao, criacao de janelas deslizantes e split temporal\n4. **Construimos** um modelo LSTM com 2 camadas em PyTorch\n5. **Treinamos** com early stopping e learning rate scheduling\n6. **Avaliamos** com MAE, RMSE e MAPE no conjunto de teste\n7. **Exportamos** o modelo, scaler e configuracao para uso em producao\n\n### Proximos passos:\n\nOs artefatos exportados (`lstm_model.pth`, `scaler.joblib`, `config.json`) serao usados pela API FastAPI + Gradio que servira as previsoes em tempo real no HuggingFace Spaces.\n\n### Limitacoes importantes:\n\n- Este modelo e **educacional** e nao deve ser usado como base para decisoes financeiras reais\n- Precos de acoes sao influenciados por fatores externos (noticias, politica, macroeconomia) que nao estao capturados nos dados historicos\n- A performance passada nao garante performance futura\n\n---\n\n*Projeto de Deep Learning aplicado a Series Temporais Financeiras | Guilherme Favaron*"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}